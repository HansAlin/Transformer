{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'pytorch' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n pytorch ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "print(os.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prevent Gradient History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3637, 0.7189, 0.0213], requires_grad=True)\n",
      "tensor([2.3637, 2.7189, 2.0213], grad_fn=<AddBackward0>)\n",
      "tensor([0.3637, 0.7189, 0.0213], requires_grad=True)\n",
      "tensor([2.3637, 2.7189, 2.0213], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3, requires_grad=True) \n",
    "print(x)\n",
    "# x.requires_grad_(False) \n",
    "# y = x.detach()\n",
    "# with torch.no_grad():\n",
    "y = x + 2\n",
    "print(y)\n",
    "print(x)\n",
    "print(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:  tensor([1., 1., 1., 1.], requires_grad=True)\n",
      "model_output:  tensor(16., grad_fn=<SumBackward0>)\n",
      "weights:  tensor([1., 1., 1., 1.], requires_grad=True)\n",
      "Grad weights tensor([1., 1., 1., 1.])\n",
      "Grad weights:  tensor([0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True) \n",
    "for epoch in range(1):\n",
    "  print('weights: ',weights)  \n",
    "\n",
    "  model_output = (weights+3).sum()  \n",
    "  print('model_output: ',model_output)\n",
    "  model_output.backward()\n",
    "  print('weights: ',weights)\n",
    "  print('Grad weights', weights.grad)\n",
    "  weights.grad.zero_()\n",
    "  print('Grad weights: ', weights.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "y_hat = w * x\n",
    "loss  = (y_hat - y)**2\n",
    "\n",
    "print(loss)\n",
    "\n",
    "# backward\n",
    "loss.backward()\n",
    "print(w.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent Autograd and Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 1.200, loss = 30.00000000 \n",
      "epoch 2: w = 1.680, loss = 4.79999924 \n",
      "epoch 3: w = 1.872, loss = 0.76800019 \n",
      "epoch 4: w = 1.949, loss = 0.12288000 \n",
      "epoch 5: w = 1.980, loss = 0.01966083 \n",
      "epoch 6: w = 1.992, loss = 0.00314574 \n",
      "epoch 7: w = 1.997, loss = 0.00050331 \n",
      "epoch 8: w = 1.999, loss = 0.00008053 \n",
      "epoch 9: w = 1.999, loss = 0.00001288 \n",
      "epoch 10: w = 2.000, loss = 0.00000206 \n",
      "Prediction after training: f(5) = 9.999\n"
     ]
    }
   ],
   "source": [
    "# linear regression manuel\n",
    "\n",
    "# f = w * x\n",
    "# f = 2*x we are looking for\n",
    "\n",
    "x = np.array([1,2,3,4], dtype=np.float32)\n",
    "y = np.array([2,4,6,8], dtype=np.float32)\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "  return w * x\n",
    "\n",
    "# loss = MSE\n",
    "def loss(y, y_pred):\n",
    "  return ((y_pred - y)**2).mean()\n",
    "\n",
    "# gradient\n",
    "# MSE 1/N * (w * x - y)**2 the loss from above\n",
    "# gradient dloss/dw = 1/N 2x (wx - y)\n",
    "def gradient(x, y,y_pred):\n",
    "  return np.dot((2*x), (y_pred - y)).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_itters = 10\n",
    "for epoch in range(n_itters):\n",
    "  # prediction = forward path\n",
    "  y_pred = forward(x)\n",
    "\n",
    "  # loss\n",
    "  l = loss(y,y_pred)\n",
    "\n",
    "  # Garadients\n",
    "  dw = gradient(x=x,y=y,y_pred=y_pred)\n",
    "\n",
    "  # update weights\n",
    "  w -= learning_rate * dw\n",
    "\n",
    "  if epoch % 1 == 0:\n",
    "    print(f'epoch {epoch + 1 }: w = {w:.3f}, loss = {l:.8f} ')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000000 \n",
      "epoch 11: w = 1.665, loss = 1.16278565 \n",
      "epoch 21: w = 1.934, loss = 0.04506890 \n",
      "epoch 31: w = 1.987, loss = 0.00174685 \n",
      "epoch 41: w = 1.997, loss = 0.00006770 \n",
      "epoch 51: w = 1.999, loss = 0.00000262 \n",
      "epoch 61: w = 2.000, loss = 0.00000010 \n",
      "epoch 71: w = 2.000, loss = 0.00000000 \n",
      "epoch 81: w = 2.000, loss = 0.00000000 \n",
      "epoch 91: w = 2.000, loss = 0.00000000 \n",
      "Prediction after training: f(5) = 5.000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# f = w * x\n",
    "# f = 2*x we are looking for\n",
    "\n",
    "x = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "  return w * x\n",
    "\n",
    "# loss = MSE\n",
    "def loss(y, y_pred):\n",
    "  return ((y_pred - y)**2).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_itters = 100\n",
    "for epoch in range(n_itters):\n",
    "  # prediction = forward path\n",
    "  y_pred = forward(x)\n",
    "\n",
    "  # loss\n",
    "  l = loss(y,y_pred)\n",
    "\n",
    "  # Garadients = backward pass\n",
    "  l.backward() # dloss/dw\n",
    "  \n",
    "  # update weights should not be part of the computational graph\n",
    "  with torch.no_grad():\n",
    "    w -= learning_rate * w.grad\n",
    "\n",
    "  # zero gradients because othervice the calculated gradients would\n",
    "  # be accumulated\n",
    "  w.grad.zero_()\n",
    "\n",
    "  if epoch % 10 == 0:\n",
    "    print(f'epoch {epoch + 1 }: w = {w:.3f}, loss = {l:.8f} ')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(2.5):.3f}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training pipeline: Model, loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: 4, features: 1\n",
      "Prediction before training: f(5) = 2.296\n",
      "epoch 1: w = 0.770, b = -0.282 loss = 18.96096230 \n",
      "epoch 11: w = 1.542, b = 2.063 loss = 0.79310715 \n",
      "epoch 21: w = 1.130, b = 3.178 loss = 2.04212499 \n",
      "epoch 31: w = 1.069, b = 2.435 loss = 1.15040660 \n",
      "epoch 41: w = 1.336, b = 1.378 loss = 0.67693180 \n",
      "epoch 51: w = 1.601, b = 0.804 loss = 0.26456171 \n",
      "epoch 61: w = 1.742, b = 0.607 loss = 0.09204076 \n",
      "epoch 71: w = 1.802, b = 0.508 loss = 0.05135943 \n",
      "epoch 81: w = 1.843, b = 0.393 loss = 0.03258922 \n",
      "epoch 91: w = 1.882, b = 0.280 loss = 0.01883996 \n",
      "epoch 101: w = 1.916, b = 0.195 loss = 0.00978376 \n",
      "epoch 111: w = 1.940, b = 0.141 loss = 0.00488339 \n",
      "epoch 121: w = 1.956, b = 0.105 loss = 0.00254761 \n",
      "epoch 131: w = 1.968, b = 0.078 loss = 0.00138982 \n",
      "epoch 141: w = 1.976, b = 0.057 loss = 0.00075984 \n",
      "epoch 151: w = 1.983, b = 0.041 loss = 0.00040750 \n",
      "epoch 161: w = 1.987, b = 0.030 loss = 0.00021568 \n",
      "epoch 171: w = 1.991, b = 0.022 loss = 0.00011437 \n",
      "epoch 181: w = 1.993, b = 0.016 loss = 0.00006114 \n",
      "epoch 191: w = 1.995, b = 0.012 loss = 0.00003280 \n",
      "epoch 201: w = 1.996, b = 0.009 loss = 0.00001757 \n",
      "epoch 211: w = 1.997, b = 0.006 loss = 0.00000938 \n",
      "epoch 221: w = 1.998, b = 0.005 loss = 0.00000500 \n",
      "epoch 231: w = 1.999, b = 0.003 loss = 0.00000267 \n",
      "epoch 241: w = 1.999, b = 0.002 loss = 0.00000143 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 251: w = 1.999, b = 0.002 loss = 0.00000076 \n",
      "epoch 261: w = 1.999, b = 0.001 loss = 0.00000041 \n",
      "epoch 271: w = 2.000, b = 0.001 loss = 0.00000022 \n",
      "epoch 281: w = 2.000, b = 0.001 loss = 0.00000012 \n",
      "epoch 291: w = 2.000, b = 0.001 loss = 0.00000006 \n",
      "epoch 301: w = 2.000, b = 0.000 loss = 0.00000003 \n",
      "epoch 311: w = 2.000, b = 0.000 loss = 0.00000002 \n",
      "epoch 321: w = 2.000, b = 0.000 loss = 0.00000001 \n",
      "epoch 331: w = 2.000, b = 0.000 loss = 0.00000001 \n",
      "epoch 341: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 351: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 361: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 371: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 381: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 391: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 401: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 411: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 421: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 431: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 441: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 451: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 461: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 471: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 481: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 491: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 501: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 511: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 521: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 531: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 541: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 551: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 561: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 571: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 581: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 591: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 601: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 611: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 621: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 631: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 641: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 651: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 661: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 671: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 681: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 691: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 701: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 711: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 721: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 731: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 741: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 751: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 761: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 771: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 781: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 791: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 801: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 811: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 821: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 831: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 841: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 851: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 861: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 871: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 881: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 891: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 901: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 911: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 921: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 931: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 941: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 951: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 961: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 971: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 981: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "epoch 991: w = 2.000, b = 0.000 loss = 0.00000000 \n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# 1) Design model (input output size, forward pass)\n",
    "# 2) Constructing loss and optimizer\n",
    "# 3) Training loop\n",
    "#    -forward\n",
    "#     -backward pass: gradient\n",
    "#     updtae weights\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# f = w * x\n",
    "# f = 2*x we are looking for\n",
    "\n",
    "# New shape when using nn models\n",
    "x = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
    "y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
    "\n",
    "x_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = x.shape\n",
    "print(f'Samples: {n_samples}, features: {n_features}') \n",
    "input_size = n_features\n",
    "out_size = n_features\n",
    "# just for manuel define\n",
    "# w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "learning_rate = 0.01\n",
    "# model prediction\n",
    "# manuel\n",
    "# def forward(x):\n",
    "#   return w * x\n",
    "\n",
    "# instead nn model we have to change shape of input\n",
    "# model = nn.Linear(input_size, out_size)\n",
    "\n",
    "# or create a costum model\n",
    "# Create a custom model\n",
    "class LienearRegression(nn.Module):\n",
    "\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "    super(LienearRegression, self).__init__()\n",
    "    # define layters\n",
    "    self.lin = nn.Linear(in_features=input_dim, out_features=output_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.lin(x)  \n",
    "\n",
    "model = LienearRegression(input_dim=input_size, output_dim=out_size)\n",
    "\n",
    "\n",
    "\n",
    "# loss = MSE\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "# we can get the weights from model.parameters()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(x_test).item():.3f}')\n",
    "\n",
    "# Training\n",
    "\n",
    "n_itters = 1000\n",
    "for epoch in range(n_itters):\n",
    "  # prediction = forward path\n",
    "  y_pred = model(x)\n",
    "\n",
    "  # loss\n",
    "  l = loss(y,y_pred)\n",
    "\n",
    "  # Garadients = backward pass\n",
    "  l.backward() # dloss/dw\n",
    "  \n",
    "  optimizer.step()\n",
    "\n",
    "  # zero gradients because othervice the calculated gradients would\n",
    "  # be accumulated\n",
    "  w.grad.zero_()\n",
    "\n",
    "  if epoch % 10 == 0:\n",
    "    [w, b] = model.parameters()\n",
    "    print(f'epoch {epoch + 1 }: w = {w[0][0].item():.3f}, b = {b[0].item():.3f} loss = {l:.8f} ')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {model(x_test).item():.3f}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linearregression torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss = 4383.5991\n",
      "epoch: 20, loss = 3269.2988\n",
      "epoch: 30, loss = 2463.4746\n",
      "epoch: 40, loss = 1880.0852\n",
      "epoch: 50, loss = 1457.2983\n",
      "epoch: 60, loss = 1150.6106\n",
      "epoch: 70, loss = 927.9451\n",
      "epoch: 80, loss = 766.1526\n",
      "epoch: 90, loss = 648.5033\n",
      "epoch: 100, loss = 562.8950\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 0) Prepare data\n",
    "x_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1)\n",
    "# x becomes a row vector\n",
    "x = torch.from_numpy(x_numpy.astype(np.float32))\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
    "# but the y values are still a column vector and \n",
    "# we need to reshape them to a row vector with one feature\n",
    "y = y.view(y.shape[0], 1)\n",
    "n_samples, n_features = x.shape\n",
    "\n",
    "# 1) Model\n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# 2) Loss and optimizer\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3) Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "  # forward pass and loss\n",
    "  y_predicted = model(x)\n",
    "  loss = criterion(input=y_predicted, target=y)\n",
    "\n",
    "  # backward pass\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  # The backward will sum up to the previous gradients, so it needs to be zeroed \n",
    "  # out explicitly before calling loss.backward() again.\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  if (epoch+1) % 10 == 0:\n",
    "    print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "\n",
    "# plot\n",
    "predicted = model(x).detach().numpy() \n",
    "plt.plot(x_numpy, y_numpy, 'ro')\n",
    "plt.plot(x_numpy, predicted, 'b')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569 30\n",
      "epoch: 10, loss = 0.5461\n",
      "epoch: 20, loss = 0.4514\n",
      "epoch: 30, loss = 0.3912\n",
      "epoch: 40, loss = 0.3494\n",
      "epoch: 50, loss = 0.3185\n",
      "epoch: 60, loss = 0.2946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 70, loss = 0.2754\n",
      "epoch: 80, loss = 0.2596\n",
      "epoch: 90, loss = 0.2464\n",
      "epoch: 100, loss = 0.2350\n",
      "y_predicted: 0.7588673830032349\n",
      "y_predicted_cls: 1.0\n",
      "accuracy: 0.9123\n"
     ]
    }
   ],
   "source": [
    "# 1) Design model (input output size, forward pass)\n",
    "# 2) Constructing loss and optimizer\n",
    "# 3) Training loop\n",
    "#    -forward\n",
    "#     -backward pass: gradient\n",
    "#     updtae weights\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split  \n",
    "######################################################################\n",
    "# 0) Prepare data\n",
    "######################################################################\n",
    "bc  = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "# scale\n",
    "# If no other arguments are given,\n",
    "# the mean and standard deviation are estimated from the data.\n",
    "sc = StandardScaler()\n",
    "sc.fit(x_train)\n",
    "x_train = sc.transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "\n",
    "x_train = torch.from_numpy(x_train.astype(np.float32))\n",
    "x_test = torch.from_numpy(x_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "# but the y values are still a column vector and\n",
    "# we need to reshape them to a row vector with one feature\n",
    "y_train = y_train.view(y_train.shape[0], 1) \n",
    "y_test = y_test.view(y_test.shape[0], 1)\n",
    "output_size = y_test.shape[1]\n",
    "\n",
    "######################################################################\n",
    "# 1) model\n",
    "######################################################################\n",
    "# f = wx + b, sigmoid at the end\n",
    "class LogisticRegression(nn.Module):\n",
    "\n",
    "  def __init__(self, n_input_features, n_output_features):\n",
    "    super(LogisticRegression, self).__init__()\n",
    "    self.lin = nn.Linear(n_input_features, n_output_features)\n",
    "\n",
    "  def forward(self, x):\n",
    "    y_pred = torch.sigmoid(self.lin(x))\n",
    "    return y_pred\n",
    "\n",
    "model = LogisticRegression(n_input_features=n_features, n_output_features=output_size)\n",
    "\n",
    "######################################################################\n",
    "# 2) loss and optimizer\n",
    "######################################################################\n",
    "learning_rate = 0.01\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "######################################################################\n",
    "# 3) Training loop\n",
    "######################################################################\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "  # forward pass and loss\n",
    "  y_pred = model(x_train)\n",
    "  loss = criterion(input=y_pred, target=y_train)\n",
    "\n",
    "  # backward pass\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  # The backward will sum up to the previous gradients, so it needs to be zeroed\n",
    "  # out explicitly before calling loss.backward() again.\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  if (epoch+1) % 10 == 0:\n",
    "    print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "\n",
    "# Evaluation should not be part of the computational graph\n",
    "with torch.no_grad():\n",
    "  y_predicted = model(x_test)\n",
    "  print(f'y_predicted: {y_predicted[0][0].item()}')\n",
    "  # Values equidistant from two integers are rounded towards the\n",
    "  #   the nearest even value (zero is treated as even)\n",
    "  y_predicted_cls = y_predicted.round() # if >= 0.5 1 else 0\n",
    "  print(f'y_predicted_cls: {y_predicted_cls[0][0].item()}')\n",
    "\n",
    "  acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "  print(f'accuracy: {acc.item():.4f}')\n",
    "\n",
    "      \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
