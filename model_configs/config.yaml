arch: Transformer
input_length: 256
n_ant: 4
training:
  batch_size: 256
  cdf: 0.7
  consecutive_deadtime: 2
  extra_gap_per_waveform: 200
  forward_skip_size: 199
  learning_rate: 0.001
  loss_fn: BCE
  ns_readout_window: 800
  permute_for_RNN_input: false
  probabilistic_sampling_ensure_min_signal_fraction: 0.3
  probabilistic_sampling_ensure_min_signal_num_bins: 3
  probabilistic_sampling_ensure_signal_region: true
  probabilistic_sampling_oversampling: 1.0
  randomize_batchitem_start_position: -1
  set_spurious_signal_to_zero: 100
  shift_signal_region_away_from_boundaries: true
  start_frac: 0.3
  test_frac: 0.1
  trigger_time: 200
  upsampling: 1
  val_frac: 0.1
transformer:
  architecture:
    GSA: false
    N: 2
    activation: relu
    antenna_type: LPDA
    by_pass: false
    d_ff: 64
    d_model: 32
    data_type: chunked
    embed_type: linear
    encoder_type: vanilla
    final_type: d_model_average_linear
    h: 4
    inherit_model: null
    input_embeddings:
      kernel_size: 3
      stride: 1
    max_pool: false
    max_relative_position: 32
    n_ant: 4
    normalization: layer
    omega: 10000
    output_shape: null
    output_size: 1
    pos_enc_type: Sinusoidal
    pre_def_dot_product: false
    pretrained: null
    projection_type: linear
    residual_type: pre_ln
    seq_len: 256
  basic:
    data_path: ''
    model: null
    model_name: Attention is all you need
    model_num: 0
    model_path: None
    model_type: base_encoder
  num of parameters:
    MACs: 0
    encoder_param: 0
    final_param: 0
    input_param: 0
    num_param: 0
    pos_param: 0
    trained_noise: 0
    trained_signal: 0
  results:
    Accuracy: 0
    Efficiency: 0
    NSE_AT_100KNRF: 0
    NSE_AT_10KNRF: 0
    NSE_AT_10KROC: 0
    Precission: 0
    TRESH_AT_10KNRF: 0
    current_epoch: 0
    energy: 0
    global_epoch: 0
    nr_area: 0
    power: 0
    roc_area: 0
    test_acc: 0
    trained: false
    training_time: 0
  training:
    batch_size: 256
    decreas_factor: 0.7
    dropout: 0
    early_stop: 100
    learning_rate: 0.001
    loss_function: BCE
    metric: Efficiency
    num_epochs: 100
    step_size: 7
    warm_up: false
