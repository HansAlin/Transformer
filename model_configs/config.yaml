arch: Transformer
data_locations:
  base_folder: /mnt/md0/data/trigger-development/rno-g
  tag_folder_veff: CDF_0.7

  # Noise for phased array
  # phased_array_noise: 
  #   prod: prod_2023.06.07 # For phased data

  # Noise for LPDA data
  high_low_noise:   
    prod: prod_2023.03.24 # For pre trigger data

  # Trigger data
  # Signal for LPDA and phased data
  pre_trig_signal:  
    prod: prod_2023.05.16 # For pre LPDA data
    # prod: prod_2023.06.12 # Fore Phased data
    

  # noiseless_signal:  # For chunked data
  #   prod: prod_2023.05.30
  # unbiased_noise: # For chunked data
  #   prod: prod_2023.02.09  


sampling:
  band:
    high: 0.23 # 0.23 LPDA, 0.22 Phased
    low: 0.08 # 0.08 LPDA, 0.096 Phased
  filter:
    order_high: 10
    order_low: 5
    type: butter
  rate: 0.5

transformer:
  architecture:
    GSA: False
    N: 2
    activation: 'relu' # Posible options: 'relu', 'gelu'
    by_pass: false
    d_ff: 64
    d_model: 32 # Have to be dividable by h
    data_type: trigger # Possible options: 'trigger', 'chunked' 'phased'
    embed_type: linear # Posible options: 'lin_relu_drop', 'lin_gelu_drop', 'linear', 'cnn', 'ViT'
    encoder_type: vanilla # Posible options: 'normal', 'none', 'bypass', 'vanilla'
    final_type: d_model_average_linear # Posible options: 'double_linear', 'single_linear', 'seq_average_linear', 'd_model_average_linear' for unknown seq_len chose 'd_model_average_linear'
    h: 4
    inherit_model: null
    input_embeddings:
      kernel_size: 3 # options for cnn: 5, 3, 1, options for ViT: 1, 2, 4
      stride: 1 # options for cnn: 2, 1, options for ViT: 1, 2, 4 Note: for ViT stride has to be equal to kernel_size
    max_pool: True
    max_relative_position: Null
    n_ant: &n_ant 4
    normalization: layer # Posible options: 'layer', 'batch'
    omega: 10000
    output_shape: null
    output_size: 1
    pos_enc_type: Sinusoidal # Posible options: 'Sinusoidal', 'Relative',  'Learnable''None',
    pre_def_dot_product: False # Posible options: True, False If true it uses the dot product defined in pytorch
    pretrained: null
    projection_type: 'linear' # Posible options: 'linear', 'cnn', 
    residual_type: 'pre_ln' # Posible options: 'pre_ln', 'post_ln'
    seq_len: &seq_len 128

  basic:
    data_path: ''
    model: null
    model_name: Attention is all you need
    model_num: 0
    model_path: None
    model_type: base_encoder
  num of parameters:
    MACs: 0
    encoder_param: 0
    final_param: 0
    input_param: 0
    num_param: 0
    pos_param: 0
    trained_noise: 0
    trained_signal: 0
  results:
    Accuracy: 0
    Efficiency: 0
    NSE_AT_100KNRF: 0
    NSE_AT_10KNRF: 0
    NSE_AT_10KROC: 0
    Precission: 0
    TRESH_AT_10KNRF: 0
    current_epoch: 0
    energy: 0
    global_epoch: 0
    nr_area: 0
    power: 0
    roc_area: 0
    test_acc: 0
    trained: false
    training_time: 0
  training:
    batch_size: &batch_size 64
    decreas_factor: 0.7
    dropout: 0
    early_stop: 100
    learning_rate: &lrn_rate 0.001
    loss_function: &loss_fn BCEWithLogits # Options BCEWithLogits, hinge, hinge_max
    warm_up: True
    metric: Efficiency
    num_epochs: 100
    step_size: 7
training:
  batch_size: *batch_size
  cdf: 0.7
  consecutive_deadtime: 2
  extra_gap_per_waveform: 200
  forward_skip_size: 199
  learning_rate: *lrn_rate
  loss_fn: *loss_fn
  ns_readout_window: 800
  permute_for_RNN_input: false
  probabilistic_sampling_ensure_min_signal_fraction: 0.3
  probabilistic_sampling_ensure_min_signal_num_bins: 3
  probabilistic_sampling_ensure_signal_region: true
  probabilistic_sampling_oversampling: 1.0
  randomize_batchitem_start_position: -1
  set_spurious_signal_to_zero: 100
  shift_signal_region_away_from_boundaries: true
  start_frac: 0.3
  test_frac: 0.1
  trigger_time: 200
  upsampling: 1
  val_frac: 0.1    

input_length: *seq_len
n_ant: *n_ant
